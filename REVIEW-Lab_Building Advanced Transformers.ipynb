{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Understand the core components of a Transformer architecture.\n",
    "- Implement a multi-head self-attention mechanism from scratch.\n",
    "- Train and evaluate a Transformer for time series prediction.\n",
    "- Handle preprocessing and scaling for time series data effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Transformer?\n",
    "\n",
    "The Transformer architecture was introduced in the paper *\"Attention Is All You Need\"*. It revolutionized natural language processing by using attention mechanisms instead of recurrence.\n",
    "\n",
    "### Key Components:\n",
    "- **Input Embedding:** Converts input tokens (or time steps) into vectors.\n",
    "- **Positional Encoding:** Injects information about the position of input tokens.\n",
    "- **Multi-Head Self-Attention:** Allows the model to focus on different parts of the input sequence.\n",
    "- **Feedforward Layers:** Process the attended information.\n",
    "- **Layer Normalization & Residual Connections:** Stabilize and speed up training.\n",
    "\n",
    "> Transformers are now widely used not only in NLP but also in time series forecasting, image recognition, and more.\n",
    "\n",
    "**Next:** You will implement parts of this architecture step-by-step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-23.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-2.1.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.78.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.13.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy>=1.26.0 (from tensorflow)\n",
      "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.3.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.10.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pillow (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading pillow-12.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.6-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-23.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (47.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.4.0-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.78.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.13.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-2.1.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
      "Downloading markdown-3.10.2-py3-none-any.whl (108 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.6-py3-none-any.whl (225 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
      "Downloading pillow-12.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.3.3-py3-none-any.whl (310 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, pyarrow, protobuf, pillow, optree, opt_einsum, numpy, mdurl, markdown, grpcio, google_pasta, gast, astunparse, absl-py, tensorboard, ml_dtypes, markdown-it-py, h5py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.4.0 astunparse-1.6.3 flatbuffers-25.12.19 gast-0.7.0 google_pasta-0.2.0 grpcio-1.78.1 h5py-3.15.1 keras-3.13.2 libclang-18.1.1 markdown-3.10.2 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.4 namex-0.1.0 numpy-2.4.2 opt_einsum-3.4.0 optree-0.18.0 pillow-12.1.1 protobuf-6.33.5 pyarrow-23.0.1 rich-14.3.3 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.3.0 werkzeug-3.1.6 wrapt-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-3.0.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-3.0.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (2.4.2)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.3 scikit-learn-1.8.0 scipy-1.17.0 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (12.1.1)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m150.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pyparsing-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow pyarrow \n",
    "%pip install pandas  \n",
    "%pip install scikit-learn \n",
    "%pip install matplotlib \n",
    "%pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 14:56:34.643470: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-02-21 14:56:34.643985: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-21 14:56:34.707948: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-21 14:56:36.334301: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-21 14:56:36.334900: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "data.to_csv('stock_prices.csv', index=False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "data = pd.read_csv('stock_prices.csv') \n",
    "data = data[['Close']].values \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads.\n",
    "\n",
    "\n",
    "# MultiHeadSelfAttention 完整讲解（含代码 + 汉语解释）\n",
    "\n",
    "这个类实现的是 **Transformer 中的多头自注意力机制（Multi-Head Self-Attention）**。\n",
    "\n",
    "它的作用是：\n",
    "\n",
    "> 让序列中的每一个 token 都能与所有 token 进行信息交互。\n",
    "\n",
    "输入和输出形状相同：\n",
    "\n",
    "```\n",
    "(batch_size, seq_len, embed_dim)\n",
    "→\n",
    "(batch_size, seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 一、完整代码\n",
    "\n",
    "```python\n",
    "class MultiHeadSelfAttention(Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        attention, _ = self.attention(query, key, value)\n",
    "\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 二、结构总览\n",
    "\n",
    "整个流程可以分为 5 步：\n",
    "\n",
    "1. 线性映射得到 Q、K、V  \n",
    "2. 拆分为多个 head  \n",
    "3. 每个 head 计算 attention  \n",
    "4. 拼接所有 head  \n",
    "5. 再做一次线性映射输出  \n",
    "\n",
    "---\n",
    "\n",
    "# 三、初始化阶段 (__init__)\n",
    "\n",
    "## 参数\n",
    "\n",
    "- `embed_dim`：embedding 总维度  \n",
    "- `num_heads`：注意力头数量  \n",
    "\n",
    "### 每个头的维度\n",
    "\n",
    "```\n",
    "projection_dim = embed_dim // num_heads\n",
    "```\n",
    "\n",
    "例如：\n",
    "\n",
    "```\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "projection_dim = 64\n",
    "```\n",
    "\n",
    "说明每个 head 处理 64 维。\n",
    "\n",
    "---\n",
    "\n",
    "## Q, K, V 线性变换\n",
    "\n",
    "```\n",
    "Q = XW_Q\n",
    "K = XW_K\n",
    "V = XW_V\n",
    "```\n",
    "\n",
    "形状：\n",
    "\n",
    "```\n",
    "(batch, seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 四、拆分为多头\n",
    "\n",
    "原始：\n",
    "\n",
    "```\n",
    "(batch, seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "reshape 后：\n",
    "\n",
    "```\n",
    "(batch, seq_len, num_heads, projection_dim)\n",
    "```\n",
    "\n",
    "transpose 后：\n",
    "\n",
    "```\n",
    "(batch, num_heads, seq_len, projection_dim)\n",
    "```\n",
    "\n",
    "这样每个 head 就可以独立计算注意力。\n",
    "\n",
    "---\n",
    "\n",
    "# 五、Attention 计算过程\n",
    "\n",
    "## 1️⃣ 点积\n",
    "\n",
    "```\n",
    "score = QK^T\n",
    "```\n",
    "\n",
    "得到：\n",
    "\n",
    "```\n",
    "(batch, heads, seq_len, seq_len)\n",
    "```\n",
    "\n",
    "表示每个 token 对所有 token 的相似度。\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ 缩放\n",
    "\n",
    "```\n",
    "scaled_score = score / sqrt(d_k)\n",
    "```\n",
    "\n",
    "防止维度过大导致 softmax 不稳定。\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ Softmax\n",
    "\n",
    "公式：\n",
    "\n",
    "\\[\n",
    "Attention(Q,K,V) = softmax(QK^T / \\sqrt{d_k}) V\n",
    "\\]\n",
    "\n",
    "得到权重矩阵。\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ 加权求和\n",
    "\n",
    "```\n",
    "output = weights V\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "(batch, heads, seq_len, projection_dim)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 六、多头拼接\n",
    "\n",
    "transpose 回来：\n",
    "\n",
    "```\n",
    "(batch, seq_len, heads, projection_dim)\n",
    "```\n",
    "\n",
    "reshape：\n",
    "\n",
    "```\n",
    "(batch, seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "再做一次线性变换：\n",
    "\n",
    "```\n",
    "output = Concat(heads) W^O\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 七、完整数学表达\n",
    "\n",
    "\\[\n",
    "MultiHead(Q,K,V) = Concat(head_1,...,head_h) W^O\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "\n",
    "\\[\n",
    "head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "# 八、时间复杂度\n",
    "\n",
    "```\n",
    "O(seq_len^2)\n",
    "```\n",
    "\n",
    "因为每个 token 都要和所有 token 计算相似度。\n",
    "\n",
    "这是 Transformer 最大瓶颈。\n",
    "\n",
    "---\n",
    "\n",
    "# 九、直观理解\n",
    "\n",
    "可以把 attention 理解为：\n",
    "\n",
    "> 一个动态学习的影响矩阵\n",
    "\n",
    "每一行代表一个 token 对所有 token 的加权组合。\n",
    "\n",
    "如果放到多智能体系统里，它就是：\n",
    "\n",
    "> 自适应的 agent 之间影响权重矩阵\n",
    "\n",
    "---\n",
    "\n",
    "# 十、工程说明\n",
    "\n",
    "这个实现是教学版本：\n",
    "\n",
    "- 没有 mask  \n",
    "- 没有 dropout  \n",
    "- 没有 causal mask  \n",
    "- 没有缓存优化  \n",
    "\n",
    "实际工程中一般使用：\n",
    "\n",
    "```python\n",
    "tf.keras.layers.MultiHeadAttention\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "如果你需要，我可以再补：\n",
    "\n",
    "- causal mask 版本  \n",
    "- 带位置编码的完整 Transformer Block  \n",
    "- FlashAttention 优化原理  \n",
    "- 或者从矩阵代数角度做严格推导\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n",
    "\n",
    "\n",
    "\n",
    "# TransformerBlock 结构完整讲解\n",
    "\n",
    "这个 `TransformerBlock` 实现的是 **标准 Transformer Encoder Block**。\n",
    "\n",
    "它由四个核心组件构成：\n",
    "\n",
    "- Multi-Head Self-Attention\n",
    "- 前馈网络（Feed Forward Network, FFN）\n",
    "- 残差连接（Residual Connection）\n",
    "- Layer Normalization\n",
    "\n",
    "输入输出形状保持一致：\n",
    "\n",
    "```\n",
    "(batch_size, seq_len, embed_dim)\n",
    "→\n",
    "(batch_size, seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 一、完整代码\n",
    "\n",
    "```python\n",
    "class TransformerBlock(Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 二、整体结构概览\n",
    "\n",
    "整个流程分为 6 步：\n",
    "\n",
    "1. Self-Attention\n",
    "2. Dropout\n",
    "3. 残差连接 + LayerNorm\n",
    "4. 前馈网络（FFN）\n",
    "5. Dropout\n",
    "6. 残差连接 + LayerNorm\n",
    "\n",
    "可以写成数学形式：\n",
    "\n",
    "\\[\n",
    "X_1 = LN(X + Attention(X))\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "Output = LN(X_1 + FFN(X_1))\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "# 三、Attention 部分\n",
    "\n",
    "```python\n",
    "attn_output = self.att(inputs)\n",
    "```\n",
    "\n",
    "这是 token 之间的信息交互。\n",
    "\n",
    "Attention 本质是：\n",
    "\n",
    "\\[\n",
    "Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\]\n",
    "\n",
    "它生成一个动态的权重矩阵，让每个 token 重新组合所有 token 的信息。\n",
    "\n",
    "---\n",
    "\n",
    "# 四、残差连接（Residual Connection）\n",
    "\n",
    "```python\n",
    "out1 = self.layernorm1(inputs + attn_output)\n",
    "```\n",
    "\n",
    "残差连接的形式是：\n",
    "\n",
    "\\[\n",
    "X + Attention(X)\n",
    "\\]\n",
    "\n",
    "作用：\n",
    "\n",
    "- 防止梯度消失\n",
    "- 允许网络退化为恒等映射\n",
    "- 使深层堆叠成为可能\n",
    "\n",
    "如果没有残差，Transformer 很难训练深层模型。\n",
    "\n",
    "---\n",
    "\n",
    "# 五、Layer Normalization\n",
    "\n",
    "LayerNorm 对每个 token 的 embedding 维度做归一化：\n",
    "\n",
    "\\[\n",
    "LN(x) = \\frac{x - \\mu}{\\sigma} \\gamma + \\beta\n",
    "\\]\n",
    "\n",
    "作用：\n",
    "\n",
    "- 稳定数值尺度\n",
    "- 加快收敛\n",
    "- 防止梯度爆炸\n",
    "\n",
    "不同于 BatchNorm，它不依赖 batch 统计量。\n",
    "\n",
    "---\n",
    "\n",
    "# 六、前馈网络（FFN）\n",
    "\n",
    "```python\n",
    "self.ffn = tf.keras.Sequential([\n",
    "    Dense(ff_dim, activation=\"relu\"),\n",
    "    Dense(embed_dim),\n",
    "])\n",
    "```\n",
    "\n",
    "结构：\n",
    "\n",
    "```\n",
    "embed_dim → ff_dim → embed_dim\n",
    "```\n",
    "\n",
    "通常：\n",
    "\n",
    "```\n",
    "ff_dim = 4 * embed_dim\n",
    "```\n",
    "\n",
    "作用：\n",
    "\n",
    "- 对每个 token 独立做非线性变换\n",
    "- 增强表达能力\n",
    "\n",
    "注意：\n",
    "\n",
    "- Attention 负责 token 之间交互\n",
    "- FFN 负责 token 内部变换\n",
    "\n",
    "如果没有 FFN，整个结构会接近线性系统。\n",
    "\n",
    "---\n",
    "\n",
    "# 七、第二次残差 + LayerNorm\n",
    "\n",
    "```python\n",
    "return self.layernorm2(out1 + ffn_output)\n",
    "```\n",
    "\n",
    "数学形式：\n",
    "\n",
    "\\[\n",
    "Output = LN(X_1 + FFN(X_1))\n",
    "\\]\n",
    "\n",
    "再次稳定训练，并允许模型在深层中保持信息流动。\n",
    "\n",
    "---\n",
    "\n",
    "# 八、信息流直观图\n",
    "\n",
    "```\n",
    "         ┌────────────────────┐\n",
    "         │ MultiHeadAttention │\n",
    "         └──────────┬─────────┘\n",
    "                    ↓\n",
    "           Residual + LayerNorm\n",
    "                    ↓\n",
    "         ┌────────────────────┐\n",
    "         │        FFN         │\n",
    "         └──────────┬─────────┘\n",
    "                    ↓\n",
    "           Residual + LayerNorm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 九、复杂度分析\n",
    "\n",
    "Attention 复杂度：\n",
    "\n",
    "\\[\n",
    "O(seq_len^2)\n",
    "\\]\n",
    "\n",
    "FFN 复杂度：\n",
    "\n",
    "\\[\n",
    "O(seq_len · embed_dim · ff_dim)\n",
    "\\]\n",
    "\n",
    "通常 Attention 是主要瓶颈。\n",
    "\n",
    "---\n",
    "\n",
    "# 十、系统视角理解\n",
    "\n",
    "整个 Block 可以写成一个非线性迭代系统：\n",
    "\n",
    "\\[\n",
    "X_{t+1} = LN(X_t + A(X_t))\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "X_{t+2} = LN(X_{t+1} + F(X_{t+1}))\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "\n",
    "- \\( A(X) \\) 是动态生成的注意力矩阵\n",
    "- \\( F(X) \\) 是逐 token 的非线性映射\n",
    "\n",
    "堆叠多个 TransformerBlock，本质是：\n",
    "\n",
    "> 多次“全局信息重组 + 局部非线性更新”\n",
    "\n",
    "---\n",
    "\n",
    "# 十一、更抽象理解\n",
    "\n",
    "如果把 token 看成 agent：\n",
    "\n",
    "- Attention = 动态生成的影响矩阵\n",
    "- FFN = agent 内部状态更新\n",
    "\n",
    "那么 TransformerBlock 就是：\n",
    "\n",
    "> 一轮网络互动 + 一轮个体策略更新\n",
    "\n",
    "这是一个动态耦合系统。\n",
    "\n",
    "---\n",
    "\n",
    "# 十二、核心总结\n",
    "\n",
    "TransformerBlock =\n",
    "\n",
    "- 动态交互结构（Attention）\n",
    "- 局部非线性变换（FFN）\n",
    "- 稳定训练机制（Residual + LayerNorm）\n",
    "\n",
    "它是 Transformer 可堆叠、可扩展、可深层训练的核心单元。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Encoder Layer \n",
    "\n",
    "Define the Encoder layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(EncoderLayer, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n",
    "\n",
    "- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement Transformer encoder \n",
    "\n",
    "Define the Transformer Encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 14:56:38.343732: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "\n",
    "class MultiHeadSelfAttention(Layer): \n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    " \n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "class TransformerBlock(Layer): \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - loss: 3.2833  \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.2043 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.1790 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.1841 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.1474 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.1502 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.1604 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.1468 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - loss: 0.0997 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.1010 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.0993 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.0963 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.1058 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.0850 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.0867 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.0790 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 1s/step - loss: 0.0884 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 1s/step - loss: 0.0560 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.0456 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.0454 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x77f755eaff80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 336ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    "\n",
    "# Prepare true values for comparison\n",
    "true_values = scaler.inverse_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Plot the predictions vs true values\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(true_values, label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.title('Predictions vs True Data (Both Scaled Back)')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - loss: 3.3423  \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 1.0807 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.6744 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.3697 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.1394 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.0693 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.0431 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0327 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0235 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0208 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - loss: 0.0191 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0163 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0144 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0156 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0136 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0114 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0145 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0096 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0104 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - loss: 0.0126 \n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 318ms/step - loss: 0.0015\n",
      "Test loss: 0.0015333832707256079\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m 31/119\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 635ms/step - loss: 0.0123 "
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here.\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "8aae4de69f29de06e63c5f2d04ef24811d42d1553c8ac316f7ad75d55f2c2d79"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
